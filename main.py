# -*- coding: utf-8 -*-
"""class_favelas_pxpx.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19CKhb9s_7C1inVCtpdvIMZPY9-XadK-p
"""

import cv2
import time
import pickle
import itertools
import numpy as np
import pandas as pd
from os import listdir
from os.path import isfile, join
from PIL import Image
import matplotlib.pyplot as plt
from imblearn.pipeline import Pipeline
from plotly import figure_factory as ff
from matplotlib.colors import ListedColormap

#Transformação em imagens
from skimage.segmentation import felzenszwalb, slic, join_segmentations, mark_boundaries
from skimage.feature import local_binary_pattern, hog, graycomatrix, graycoprops, shape_index
from skimage.measure import label
from skimage.util import img_as_ubyte, view_as_windows

#Métricas
from tensorflow.keras.metrics import MeanIoU
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score, precision_score, recall_score

#Modelos
import lightgbm as lgb
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

#Modelos GPU
#from cuml.linear_model import LogisticRegression
#from cuml.naive_bayes import MultinomialNB
#from cuml.cluster import KMeans
#from cuml.ensemble import RandomForestClassifier

#Acesso ao Drive
from google.colab import drive
drive.mount('/content/drive')

#Criando uma legenda de cores para favelas e não favelas
mapa_cores = {
    0: (0, 0, 0, 0),        # vazio
    1: (1, 0, 0, 1)         # vermelho
}

lista_cores = [mapa_cores[x] for x in [0,1]]
cmap_ = ListedColormap(lista_cores)

#Função para ler imagens em uma pasta
def read_mask_and_image(image_path,mask_path):
  image_list = []
  mask_list  = []

  files_image = sorted(listdir(image_path))
  files_mask  = sorted(listdir(mask_path))

  for file_img,file_mask in zip(files_image,files_mask):
    if isfile(join(image_path, file_img)):
      image = Image.open(join(image_path, file_img))
      image = np.array(image)
      mask  = Image.open(join(mask_path, file_mask))
      mask  = np.array(mask)
      #mask  = mask[:,:,0]
      mask[np.where(mask != 0)] = 1
      image_list.append(image)
      mask_list.append(mask)

  return image_list, mask_list

#Caminhos
#Alterar os caminhos para escolher onde salvar os resultados de cada etapa
image_path   = "/content/drive/MyDrive/Dados/Projeto_grade_Com_Aglomerados/amostras_termianadas/regioes amostradas imagens/"
mask_path    = "/content/drive/MyDrive/Dados/Projeto_grade_Com_Aglomerados/amostras_termianadas/regioes amostradas mascaras/"
vectors_path = '/content/drive/MyDrive/Dados/imagens_zona_sul/vetores salvos/'
results_path = '/content/drive/MyDrive/Dados/imagens_zona_sul/resultados modelos/'

#Lendo imagens e plotando imagens e máscaras lado a lado
image_list,mask_list = read_mask_and_image(image_path,mask_path)

for image,mask in zip(image_list,mask_list):
  fig,ax = plt.subplots(1,3,figsize=(10,10))
  ax[0].imshow(image)
  ax[0].set_title("Imagem")
  ax[0].axis('off')
  ax[1].imshow(mask,cmap=cmap_)
  ax[1].set_title("Máscara")
  ax[1].axis('off')
  ax[2].imshow(image)
  ax[2].imshow(mask,alpha=0.5,cmap=cmap_)
  ax[2].set_title("Imagem e Máscara")
  ax[2].axis('off')


  plt.show()

#Função para criar os Superpixels
def superpixel_image_AllBands(image, min_number_pixels = 17, distance = 'mean', verbose=False):
    work_image = np.zeros((image.shape[0],image.shape[1],3),dtype=float)
    work_image[:,:,0] = image[:,:,0]
    work_image[:,:,1] = image[:,:,1]
    work_image[:,:,2] = image[:,:,2]

    s2 = felzenszwalb(work_image, scale=1, sigma=0.8, min_size=int(min_number_pixels))
    if verbose:
        print(f'Felzenszwalb number of segments: {len(np.unique(s2))}')

    s1 = slic(work_image, n_segments=int((work_image.shape[0]*work_image.shape[1]/(min_number_pixels*4))), compactness=3, sigma=1, start_label=1)
    if verbose:
        print(f'SLIC number of segments: {len(np.unique(s1))}')

    return s1,s2,work_image

#Função para extrair as Features pelo método de HOG
def extract_hog_features(image,orientations=9,pixels_cell=(10,10),cells_block=(3,3)):
  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convertendo a imagem para preto e branco

  # Definir parâmetros para HOG
  cell_size       = pixels_cell
  block_size      = cells_block
  nbins           = orientations
  pixels_per_cell = cell_size
  cells_per_block = block_size

  #A função realiza pequenas convoluções e calcula o HOG dentro de cada janela convolucional
  #É feito desse modo pois o HOG retorna um vetor de features para uma única imagem, não para cada pixel da imagem
  pad_size     = ((cell_size[0]//2,cell_size[0]//2),(cell_size[1]//2,cell_size[1]//2)) # Adicionar bordas à imagem para calcular HOG nas bordas
  padded_image = np.pad(gray_image,pad_size,mode='constant',constant_values=0) #Vetorização da imagem com bordas

  window_shape = (cell_size[0]*block_size[0],cell_size[1]*block_size[1]) # Extrair janelas da imagem
  windows = view_as_windows(padded_image,window_shape)

  hog_vectors = np.zeros((gray_image.shape[0],gray_image.shape[1],nbins*block_size[0]*block_size[1])) # Inicializar lista para armazenar os vetores de HOG locais

  for i in range(windows.shape[0]): # Calcular HOG para cada janela
      for j in range(windows.shape[1]):
          window     = windows[i, j]
          hog_vector = hog(window, pixels_per_cell=pixels_per_cell,cells_per_block=cells_per_block,orientations=nbins,block_norm='L2-Hys',feature_vector=True)
          hog_vectors[i, j, :] = hog_vector

  return hog_vectors

#Função para extrair as Features da matriz GLCM
def extract_GLCM_features(image, window_size=9, distances=[1], angles=[0]):
    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #Convertendo a imagem para preto e branco

  #A função realiza pequenas convoluções na imagem (mesmo problema do HOG)
    pad_size = window_size // 2
    padded_image = np.pad(image_gray, pad_width=pad_size, mode='constant', constant_values=0) #Adicionando bordas a imagem

    features_shape = (image_gray.shape[0], image_gray.shape[1], 6)  #vetor para armazenar 6 GLCM features
    glcm_features = np.zeros(features_shape)

    windows = view_as_windows(padded_image, (window_size, window_size))

    for i in range(image_gray.shape[0]):
        for j in range(image_gray.shape[1]):
            window = windows[i, j]
            glcm = graycomatrix(window, distances=distances, angles=angles, levels=256, symmetric=True, normed=True) #Calculando a GLCM para a convolução

            #Calculando 6 características a partir da Matriz GLCM
            contrast      = graycoprops(glcm,'contrast')[0,0]
            dissimilarity = graycoprops(glcm,'dissimilarity')[0,0]
            homogeneity   = graycoprops(glcm,'homogeneity')[0,0]
            ASM           = graycoprops(glcm,'ASM')[0,0]
            energy        = graycoprops(glcm,'energy')[0,0]
            correlation   = graycoprops(glcm,'correlation')[0,0]

            glcm_features[i,j,0] = contrast
            glcm_features[i,j,1] = dissimilarity
            glcm_features[i,j,2] = homogeneity
            glcm_features[i,j,3] = ASM
            glcm_features[i,j,4] = energy
            glcm_features[i,j,5] = correlation

    return glcm_features

#Função para calcular as estatísticas do superpixel
def stats_img(image,label):
  superpixel_stats = []

  im_rp  = image.reshape((image.shape[0]*image.shape[1],image.shape[2]))
  sli_1d = np.reshape(label,-1)
  uni    = np.unique(sli_1d)
  uu_1 = uu_2 = uu_3 = uu_4 = uu_5 = np.zeros(im_rp.shape)

  for i in uni: #Média do superpixel
    loc        = np.where(sli_1d==i)[0]
    mean_sp    = np.mean(im_rp[loc,:],axis=0)
    uu_1[loc,:]  = mean_sp
  superpixel_mean = np.reshape(uu_1,[image.shape[0],image.shape[1],image.shape[2]]).astype('uint8')

  for i in uni: #Desvio padrão do superpixel
    loc    = np.where(sli_1d==i)[0]
    std_sp = np.std(im_rp[loc,:],axis=0)
    uu_2[loc,:]  = std_sp
  superpixel_std = np.reshape(uu_2,[image.shape[0],image.shape[1],image.shape[2]]).astype('uint8')

  for i in uni: #Mediana do superpixel
    loc       = np.where(sli_1d==i)[0]
    median_sp = np.median(im_rp[loc,:],axis=0)
    uu_3[loc,:]  = median_sp
  superpixel_median = np.reshape(uu_3,[image.shape[0],image.shape[1],image.shape[2]]).astype('uint8')

  for i in uni: #Máximo do superpixel
    loc    = np.where(sli_1d==i)[0]
    max_sp = np.max(im_rp[loc,:],axis=0)
    uu_4[loc,:]  = max_sp
  superpixel_max = np.reshape(uu_4,[image.shape[0],image.shape[1],image.shape[2]]).astype('uint8')

  for i in uni: #Mínimo do superpixel
    loc    = np.where(sli_1d==i)[0]
    min_sp = np.min(im_rp[loc,:],axis=0)
    uu_5[loc,:]  = min_sp
  superpixel_min = np.reshape(uu_5,[image.shape[0],image.shape[1],image.shape[2]]).astype('uint8')

  superpixel_stats.append(superpixel_mean)
  superpixel_stats.append(superpixel_std)
  superpixel_stats.append(superpixel_median)
  superpixel_stats.append(superpixel_max)
  superpixel_stats.append(superpixel_min)

  return superpixel_stats

#Extrai features da imagem
def extract_features(image,label_fz,label_SLIC,SSI=True,stats=True,lbp=True,hog_feature=True,glcm=True,verbose=True):
  image_gray     = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
  feature_vector = image

  if verbose:
    fig,axs = plt.subplots(4,5,figsize=(15, 12))
    axs[0,0].imshow(image)
    axs[0,0].set_title("Original Image",fontsize=10)
    axs[0,0].axis('off')

  if SSI:
    indices = shape_index(image_gray) #SSI
    feature_vector = np.dstack((feature_vector,indices))
    if verbose:
      axs[0,1].imshow(indices)
      axs[0,1].set_title("SSI",fontsize=10)
      axs[0,1].axis('off')

  if stats:
    stats_fz   = stats_img(image,label_fz) #Felzenszwalb
    stats_slic = stats_img(image,label_SLIC) #SLIC
    for i in range(0,len(stats_fz)):
      feature_vector = np.dstack((feature_vector,stats_fz[i]))
    for i in range(0,len(stats_fz)):
      feature_vector = np.dstack((feature_vector,stats_slic[i]))

    if verbose:
      felzenszwalb_legend = ["Felzenszwalb Mean","Felzenszwalb SD","Felzenszwalb Median","Felzenszwalb Max","Felzenszwalb Min"]
      SLIC_legend = ["SLIC Mean","SLIC SD","SLIC Median","SLIC Max","SLIC Min"]

      for i in range(0,len(stats_fz)): #Felzenszwalb
        axs[2,i].imshow(stats_fz[i])
        axs[2,i].set_title(felzenszwalb_legend[i],fontsize=10)
        axs[2,i].axis('off')

      for i in range(0,len(stats_fz)): #SLIC
        axs[3,i].imshow(stats_slic[i])
        axs[3,i].set_title(SLIC_legend[i],fontsize=10)
        axs[3,i].axis('off')

  if lbp:
    lbp_feature    = local_binary_pattern(image_gray,8,5) #LBP
    feature_vector = np.dstack((feature_vector,lbp_feature))
    if verbose:
      axs[0,2].imshow(lbp_feature)
      axs[0,2].set_title("LBP",fontsize=10)
      axs[0,2].axis('off')

  if hog_feature:
    hog_feature  = extract_hog_features(image) #HOG
    feature_vector = np.dstack((feature_vector,hog_feature))
    if verbose:
      hog_image = hog(image_gray, pixels_per_cell=(10,10),cells_per_block=(3,3),orientations=9,block_norm='L2-Hys',visualize=True,feature_vector=False)
      axs[0,3].imshow(hog_image[1])
      axs[0,3].set_title("HOG",fontsize=10)
      axs[0,3].axis('off')

  if glcm:
    glcm_feature = extract_GLCM_features(image) #GLCM
    feature_vector = np.dstack((feature_vector,glcm_feature))
    if verbose:
      GLCM_legend = ["GLCM - contrast","GLCM - dissimilarity","GLCM - homogeneity","GLCM - ASM","GLCM - energy","GLCM - correlation"]
      axs[0,4].imshow(glcm_feature[:,:,0])
      axs[0,4].set_title(GLCM_legend[0],fontsize=10)
      axs[0,4].axis('off')
      for i in range(1,6):
        axs[1,i-1].imshow(glcm_feature[:,:,i])
        axs[1,i-1].set_title(GLCM_legend[i],fontsize=10)
        axs[1,i-1].axis('off')

  if verbose:
    plt.subplots_adjust(wspace=0.4, hspace=0.6)
    plt.show()

  return feature_vector

#Adiciona features da imagem
def add_features(image_list,mask_list,verbose=True,SSI=True,stats=True,lbp=True,hog_feature=True,glcm=True):
  SLIC_superpixel         = []
  felzenszwalb_superpixel = []
  features_list           = []

  for image in image_list:
    SLIC,felzenszwalb,work_image = superpixel_image_AllBands(image,verbose=verbose)
    SLIC_superpixel.append(SLIC)
    felzenszwalb_superpixel.append(felzenszwalb)

  for image,felzenszwalb,SLIC in zip(image_list,felzenszwalb_superpixel,SLIC_superpixel):
    features = extract_features(image,felzenszwalb,SLIC,verbose=verbose,SSI=SSI,stats=stats,lbp=lbp,hog_feature=hog_feature,glcm=glcm)
    features_list.append(features)

  return features_list,mask_list

#Adicionando features a imagem
image_features,masks_list = add_features(image_list,mask_list,verbose=True)

#Salvando os vetores de features
with open(os.path.join(vectors_path, 'image_features_test.pkl'), 'wb') as f:
    pickle.dump(image_features, f)

with open(os.path.join(vectors_path, 'masks_list_test.pkl'), 'wb') as f:
    pickle.dump(masks_list, f)

#Ler vetores salvos
image_features = pickle.load(open(os.path.join(vectors_path, 'image_features_test.pkl'), 'rb'))
masks_list = pickle.load(open(os.path.join(vectors_path, 'masks_list_test.pkl'), 'rb'))

#Função para separar o conjunto de imagens em treino e teste com validação cruzada
def train_test_kfold(features_final_lista, masks_final_lista, n_splits=5):
    if len(features_final_lista) != len(masks_final_lista):
        raise ValueError("A lista de features e a lista de masks devem ter o mesmo tamanho.")

    treino_X = []
    treino_Y = []
    teste_X  = []
    teste_Y  = []

    kf = KFold(n_splits=n_splits) #Implementação do k-fold para fazer as divisões

    for train_index, test_index in kf.split(features_final_lista):
        #Concatenando treino e vetorizando a imagem
        X_treino = np.concatenate([features_final_lista[i].reshape((features_final_lista[i].shape[0]*features_final_lista[i].shape[1],features_final_lista[i].shape[2])) for i in train_index], axis=0)
        Y_treino = np.concatenate([masks_final_lista[i].reshape((masks_final_lista[i].shape[0]*masks_final_lista[i].shape[1])) for i in train_index], axis=0)

        #Concatenando teste e vetorizando a imagem
        X_teste = np.concatenate([features_final_lista[i].reshape((features_final_lista[i].shape[0]*features_final_lista[i].shape[1],features_final_lista[i].shape[2])) for i in test_index], axis=0)
        Y_teste = np.concatenate([masks_final_lista[i].reshape((masks_final_lista[i].shape[0]*masks_final_lista[i].shape[1])) for i in test_index], axis=0)

        treino_X.append(X_treino)
        treino_Y.append(Y_treino)
        teste_X.append(X_teste)
        teste_Y.append(Y_teste)

    return treino_X, treino_Y, teste_X, teste_Y

#Normalizar os vetores?
#Divide o conjunto de imagens em treino e teste pelo método k-fold
X_train, y_train, X_test, y_test = train_test_kfold(image_features, masks_list)
print(f"Quantidade de imagens de treino: {len(X_train)}")
print(f"Quantidade de imagens de teste: {len(X_test)}")
print(f"Shape do treino X: {X_train[0].shape}")
print(f"Shape do treino Y: {y_train[0].shape}")
print(f"Shape do teste X: {X_test[0].shape}")
print(f"Shape do teste Y: {y_test[0].shape}")

#Calculando as proporções em treino e teste
for train,test in zip(y_train,y_test):
  num_negative_train = np.count_nonzero(train == 0)
  num_positive_train = np.count_nonzero(train != 0)
  num_negative_test  = np.count_nonzero(test == 0)
  num_positive_test  = np.count_nonzero(test != 0)
  prop_positive_train = num_positive_train / (num_negative_train + num_positive_train)
  prop_negative_train = num_negative_train / (num_negative_train + num_positive_train)

  prop_positive_test  = num_positive_test / (num_negative_test + num_positive_test)
  prop_negative_test  = num_negative_test / (num_negative_test + num_positive_test)

  print(f"----------------------")
  print(f"Proporção de pixels positivos no treino: {prop_positive_train}")
  print(f"Proporção de pixels negativos no treino: {prop_negative_train}")
  print(f"Proporção de pixels positivos no teste: {prop_positive_test}")
  print(f"Proporção de pixels negativos no teste: {prop_negative_test}")

#Salvando os vetores de treino e teste
with open(os.path.join(vectors_path, 'X_train.pkl'), 'wb') as f:
    pickle.dump(X_train, f)

with open(os.path.join(vectors_path, 'y_train.pkl'), 'wb') as f:
    pickle.dump(y_train, f)

with open(os.path.join(vectors_path, 'X_test.pkl'), 'wb') as f:
    pickle.dump(X_test, f)

with open(os.path.join(vectors_path, 'y_test.pkl'), 'wb') as f:
    pickle.dump(y_test, f)

#Lendo arquvios salvos
X_train = pickle.load(open(os.path.join(vectors_path, 'X_train.pkl'), 'rb'))
y_train = pickle.load(open(os.path.join(vectors_path, 'y_train.pkl'), 'rb'))
X_test  = pickle.load(open(os.path.join(vectors_path, 'X_test.pkl'), 'rb'))
y_test  = pickle.load(open(os.path.join(vectors_path, 'y_test.pkl'), 'rb'))

#Definindo modelos e Grid para cada modelo
models=[]

name = 'RandomForest'
model = RandomForestClassifier(random_state=42,criterion='entropy')
grid = {
    'model__max_depth':[int(x) for x in range(2,3)],
    'model__n_estimators': [int(x) for x in np.linspace(start=300,stop=700,num=3)],
#    'model__criterion': ('gini', 'entropy'),
    'model__class_weight': [{0:1, 1:3},{0:1, 1:5}],
}

models.append({'name': name, 'model': model, 'grid': grid})

name = 'XGBoost'
model = XGBClassifier(objective='binary:logistic',device='cuda',subsample=0.5)
grid = {
    'model__max_depth': [int(x) for x in np.linspace(start=2, stop=3,num=2)],
    'model__n_estimators': [int(x) for x in np.linspace(start=400, stop=650, num=3)],
    'model__learning_rate': [int(x) for x in np.linspace(start=0.2, stop=1,num=4)],
#    'model__min_child_weight': [int(x) for x in np.linspace(start=3, stop=5,num=2)],
#    'model__scale_pos_weight': [int(x) for x in np.linspace(start=2, stop=5,num=2)],
}

models.append({'name': name, 'model': model, 'grid': grid})

name = 'LGBMClassifier'
model = lgb.LGBMClassifier(random_state=42,objective='binary')
grid = {
#    'model__num_leaves': [int(x) for x in np.linspace(start=2, stop=10,num=3)],
    'model__n_estimators': [int(x) for x in np.linspace(start=400, stop=1000, num=3)],
#    'model__reg_alpha': [0, 1e-1, 1],
#    'model__reg_lambda': [0, 1e-1, 1],
    'model__max_depth': [int(x) for x in np.linspace(start=2, stop=3,num=2)],
    'model__class_weight': [{0:1, 1:2},{0:2, 1:1}],
#    'model__learning_rate': [int(x) for x in np.linspace(start=0.2, stop=1,num=4)],
}

models.append({'name': name, 'model': model, 'grid': grid})

name = 'Kmeans'
model = KMeans(n_clusters=2,random_state=42)
grid = {
#    'model__n_clusters': [int(x) for x in np.linspace(start=1, stop=2, num=2)],
    'model__max_iter': [int(x) for x in np.linspace(start=50, stop=500, num=10)],
#    'model__algorithm': ['lloyd', 'elkan'],
}

models.append({'name': name, 'model': model, 'grid': grid})

name = 'Naive-Bayes'
model = GaussianNB()
grid = {
#    'model__n_features_in_': [int(x) for x in np.linspace(start=1, stop=20, num=3)],
    'model__epsilon_': [int(x) for x in np.linspace(start=1, stop=50, num=5)],
}

models.append({'name': name, 'model': model, 'grid': grid})

name = 'LogisticRegression'
model = LogisticRegression(random_state=42)
grid = {
    'model__penalty': ['l2', 'l1', 'elasticnet'],
#    'model__solver': ['lbfgs', 'sag'],
}

models.append({'name': name, 'model': model, 'grid': grid})

name = 'MLPClassifier'
model = MLPClassifier(max_iter=20,random_state=42)
grid = {
    'model__max_iter': [int(x) for x in np.linspace(start=200, stop=300,num=4)],
}

models.append({'name': name, 'model': model, 'grid': grid})

#Função para treinar e testar os modelos
def train_test(mat, X, y):
    model = mat['model']
    name = mat['name']
    grid = mat['grid']

    print(f'Começando {name}.')
    ini = time.time()
    pipe = Pipeline([('model', model)])

    gscv = GridSearchCV(
        pipe,
        param_grid = grid,
        scoring    = ['f1','jaccard'],
        refit      = 'jaccard',
        cv         = 3,
        verbose    = True,
        n_jobs     = 10,
    )

    rscv = gscv.fit(X,y)
    fim = time.time()

    print(f'Término {name} | Score: {round(rscv.best_score_,3)} | Time: {rscv.refit_time_}')
    return {'name':name, 'object': rscv}

#Realizando o GridSearchCV para cada conjunto de treino do K-fold
#Salvar o modelo, o número da iteração, e os vetores resultantes
models_result = []
times_result  = []

for X,y in zip(X_train,y_train):
  info_model  = []
  info_values = []
  info_time   = []

  grid_result = [train_test(mat, X, y) for mat in models]
  models_result.append(grid_result)

  for index, mod  in enumerate(grid_result):
    info_model.append(grid_result[index]['name'])
    info_values.append(round(grid_result[index]['object'].best_score_,3))
    info_time.append(grid_result[index]['object'].refit_time_)

  times_result.append({'Model': info_model, 'Score': info_values, 'Time': info_time})
  dados = pd.DataFrame({'Model': info_model, 'Score': info_values, 'Time': info_time})
  fig = ff.create_table(dados, height_constant=20)
  fig.show()

#Salvando os resultados do Grid e tempos dos modelos
with open(os.path.join(results_path, 'models_result.pkl'), 'wb') as f:
    pickle.dump(models_result, f)

with open(os.path.join(results_path, 'times_result.pkl'), 'wb') as f:
    pickle.dump(times_result, f)

#Lendo arquvios salvos
models_result = pickle.load(open(os.path.join(results_path, 'models_result.pkl'), 'rb'))
times_result  = pickle.load(open(os.path.join(results_path, 'times_result.pkl'), 'rb'))

#Faz o ajuste dos modelos com melhores parâmetros e retorna as métricas de predição
def predict_best_model(model_grid_list, X_train, y_train, X_test, y_test, verbose=True):
  preds      = []
  model_info = []
  time_info  = []

  score_table = pd.DataFrame(columns=['Model', 'Accuracy','F1 Score','Precision', 'Recall']) #columns=['Model','F1 Grid', 'Accuracy','F1 Score','Precision', 'Recall']
  roc_curve_df   = pd.DataFrame(columns=['classificadores', 'fpr','tpr','auc'])

  for grid in model_grid_list:
    model = grid['object'].best_estimator_

    if verbose:
      print(f'Começando {grid["name"]}.')

    start_train = time.time()
    model.fit(X_train, y_train) #Treinamento do modelo
    end_train = time.time()

    train_time = end_train - start_train

    start_test = time.time()
    y_pred = model.predict(X_test) #Predição do modelo
    end_test = time.time()

    test_time = end_test - start_test

    time_info.append({'name': grid['name'],'train':train_time, 'test':test_time})

    preds.append({'name': grid['name'], 'pred': y_pred})

    acc = round(accuracy_score(y_test, y_pred),3)
    f1  = round(f1_score(y_test, y_pred),3)
    pc  = round(precision_score(y_test, y_pred),3)
    re  = round(recall_score(y_test, y_pred),3)

    if verbose:
      print(f'Término {grid["name"]} | F1 Score: {f1} | Train Time: {train_time} | Test Time: {test_time}')

    score_table = score_table._append({'Model'    :grid['name'],
                                       #'F1 Grid'  :info_values,
                                       'Accuracy' :acc,
                                       'F1 Score' :f1,
                                       'Precision':pc,
                                       'Recall'   :re}, ignore_index=True)

    if verbose:
      print(f'Confusion Matrix: \n{confusion_matrix(y_test, y_pred)}')

    model_info.append({'name': grid['name'], 'model': model})

    fpr, tpr, _  = roc_curve(y_test,  y_pred) #Verdadeiro positivo e falso positivo
    auc          = roc_auc_score(y_test, y_pred) #Área abaixo da curva
    roc_curve_df = roc_curve_df._append({'model' :grid['name'],
                                         'fpr'   :fpr,
                                         'tpr'   :tpr,
                                         'auc'   :auc}, ignore_index=True)

  return preds, model_info, time_info, score_table, roc_curve_df

#Faz a tabela final de métricas
def plot_metric_table(metric_df):
  fig = ff.create_table(metric_df, height_constant=20)
  fig.show()

  return fig

#Faz o gráfico da curva ROC
def plot_roc_curve(roc_df):
  fig = plt.figure(figsize=(12,6))

  for k in roc_df.index:
      plt.plot(roc_df.loc[k]['fpr'],
               roc_df.loc[k]['tpr'],
               label="{}, AUC={:.3f}".format(k, roc_df.loc[k]['auc']))

  plt.plot([0,1], [0,1], color='orange', linestyle='--')

  plt.xticks(np.arange(0.0, 1.1, step=0.1))
  plt.xlabel("Taxa de Falso Positivo", fontsize=15)

  plt.yticks(np.arange(0.0, 1.1, step=0.1))
  plt.ylabel("Taxa de Verdadeiro Positivo", fontsize=15)

  plt.title('Curva ROC', fontweight='bold', fontsize=15)
  plt.legend(prop={'size':13}, loc='lower right')

  plt.show()

  return fig

#Realizando as predições com os melhores modelos
final_results  = []
final_predicts = []
final_time     = []
final_models   = []

for models_grid,X,y,X_t,y_t in zip(models_result,X_train,y_train,X_test,y_test):
  preds, models_info, times, score_df, roc_df = predict_best_model(models_grid, X, y, X_t, y_t)

  final_results.append(score_df)
  final_predicts.append(preds)
  final_time.append(times)
  final_models.append(models_info)

  fig = plot_metric_table(score_df)
  fig.show()

  fig = plot_roc_curve(roc_df)
  fig.show()

#Salvando tudo
with open(os.path.join(results_path, 'final_results.pkl'), 'wb') as f:
    pickle.dump(final_results, f)

with open(os.path.join(results_path, 'final_predicts.pkl'), 'wb') as f:
    pickle.dump(final_predicts, f)

with open(os.path.join(results_path, 'final_time.pkl'), 'wb') as f:
    pickle

with open(os.path.join(results_path, 'final_models.pkl'), 'wb') as f:
    pickle.dump(final_models, f)

#Lendo tudo
final_results = pickle.load(open(os.path.join(results_path, 'final_results.pkl'), 'rb'))
final_predicts = pickle.load(open(os.path.join(results_path, 'final_predicts.pkl'), 'rb'))
final_time = pickle.load(open(os.path.join(results_path, 'final_time.pkl'), 'rb'))
final_models = pickle.load(open(os.path.join(results_path, 'final_models.pkl'), 'rb'))

#Reconstroi a predição
def prediction_to_image(pred, mask):
  img_pred = np.reshape(pred, (mask.shape[0], mask.shape[1]))

  fig, axs = plt.subplots(1, 2, figsize=(10, 5))
  axs[0].imshow(img_pred)
  axs[0].set_title('Predição')
  axs[0].axis('off')

  axs[1].imshow(mask)
  axs[1].set_title('Máscara')
  axs[1].axis('off')
  plt.show()

  return fig